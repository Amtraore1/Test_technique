# Optimisation d‚ÄôInfrastructure et recommendation

## Objectif

Ce projet est une solution modulaire de monitoring et d‚Äôanalyse d‚Äôinfrastructure pour une PME. Il permet de :

- Ingestion de donn√©es syst√®me au format JSON  
- D√©tection d‚Äôanomalies (CPU, latence, etc.)  
- G√©n√©ration de recommandations via le mod√®le LLM 
- Export des r√©sultats dans un fichier json  

---

## Technologies Utilis√©es

| Outil / Lib         | R√¥le |
|---------------------|------|
| Python 3.10     | Langage principal |
| `pydantic`           | Validation des donn√©es |
| `mistralai`          | Appel au mod√®le Mistral |
| `dotenv`             | Gestion des variables d'environnement |
| `json`               | Traitement des donn√©es |

---

##  Architecture du Projet
1- analyse.py # D√©tection des anomalies techniques   
2- ingestion.py # Lecture et validation des donn√©es JSON  
3- main.py # Pipeline principal  
4- recommendation.py # Appel √† Mistral pour g√©n√©rer les recommandations  
5- data/  
    a-rapport.json # Donn√©es brutes d'entr√©e  
    b-rapport_final.json # R√©sultat final structur√©  

---

##  Fonctionnement par √âtapes

### Ingestion (ingestion.py)

Lit un fichier .json contenant les m√©triques syst√®me  
Valide les champs avec pydantic  

### Analyse (analyse.py)

V√©rifie les valeurs au-del√† des seuils critiques  
Retourne un dictionnaire des anomalies d√©tect√©es  

### Recommandation (recommendation.py)

Formate un prompt avec les anomalies 
Utilise l‚ÄôAPI mistralai pour interroger un mod√®le Mistral  
Retourne des recommandations ligne par ligne  

### Pipeline (main.py)

G√®re l‚Äôex√©cution compl√®te du processus  
G√®re la lecture .env et sauvegarde le r√©sultat final dans rapport_final.json  

##  pendant le test en pr√©sentiel
Pendant  le test en pr√©sentiel, j'ai voulu ajouter une nouvelle √©tape pour analyser l‚Äôhistorique des donn√©es (CPU, latence, erreurs, temp√©rature, etc.).
Cette analyse utilise une r√©gression lin√©aire pour d√©tecter les tendances √† la hausse ou √† la baisse, ce qui permet d‚Äôanticiper les d√©faillances et d‚Äôoptimiser la gestion des ressources.

Les r√©sultats devront etre int√©gr√©s dans le prompt envoy√© au LLM afin de proposer des recommandations plus pr√©cises et pr√©ventives.

##  Apr√®s le test en pr√©sentiel

Suite au test r√©alis√© en pr√©sentiel, j‚Äôai modifi√© le **prompt de g√©n√©ration** afin d'y int√©grer une **analyse pr√©dictive des tendances** bas√©es sur l‚Äôhistorique des m√©triques.

-  **Avantage** : Tr√®s simple √† impl√©menter (modification du prompt uniquement, sans changement d'architecture)
-  **Inconv√©nient** : Le traitement est **extr√™mement long** (plus de 5 heures d'ex√©cution) d√ª √† la taille des donn√©es et la g√©n√©ration LLM s√©quentielle

üìÑ Le r√©sultat de cette nouvelle version se trouve ici :  
`data/rapport_apres_test.json`
)
